{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi, I'm Liam","text":"<p>I'm a machine learning engineer with a decade of experience. I've worked at a successful startup and a top consulting firm, and now I work as an independent consultant helping companies apply the latest in machine learning and AI to their businesses.</p>"},{"location":"#ai-and-ml-consulting","title":"AI and ML Consulting","text":"<p>You can find more about the services that I offer here.  If you're interested in working with me, send me an email!</p>"},{"location":"#work-experience","title":"Work Experience","text":"<ul> <li>Google Cloud, Applied AI (current) - Working on enterprise applications of AI</li> <li>BCG Gamma - The machine larning and data science practice area of BCG.</li> <li>Auth0 - successful unicorn B2B SaaS startup.</li> <li>Independent consultant (Canto Analytics) - Algorithmic trading model development and testing</li> <li>Independent consultant (Cervos) - Designed and trained a Reinforcement Learning agent to optimize control of machinery at a food processing plant, using model-based and model-free RNN-based architecture.</li> </ul>"},{"location":"other_writing/","title":"Other Writing","text":"<p>2024 - Down-to-Earth Satellite Remote Detection: A Human-Centric Approach to Geospatial Intelligence</p> <p>2021 - Software Engineers Build the Central Nervous System of Enterprise AI</p> <p>2020 - Shifting AI Software Development into High Gear</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#footyai-2025","title":"FootyAI (2025)","text":"<p>Multi-agent adversarial self-play RL on a virtual soccer pitch. Emergent tactics and cooperation between independent agents. </p>"},{"location":"projects/#text-diffusion-experiments-2025","title":"Text Diffusion Experiments (2025)","text":"<p>Miniature experiments and research in text diffusion and controled generation. </p>"},{"location":"projects/#deep-algo-trading-2016","title":"Deep Algo Trading (2016)","text":"<p>Demonstrations of Tensorflow (when it was in beta!) applied to the field of algorithmic trading. Structured as an educational course that walks from simple regression, through CNNs, RNNs, and into policy networks (RL). </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/author/team/","title":"The Material Team","text":"<p>A small group of people dedicated to making writing documentation easy, if not outright fun! Here are some of the things we have blogged about:</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/","title":"5 Archetypes of Knowledge-Intensive Applications","text":""},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#it-doesnt-always-have-to-be-rag","title":"It doesn't always have to be RAG.","text":"<p>The current GenAI boom has seen the rise of the Retrieval Augmented Generation (RAG) architecture, which promises to enhance the functionality of chatbots by providing them with context from a knowledge base. In some circles, RAG has become synonymous with LLM-based applications. </p> <p>I think this is a mistake, and I believe that many of the most valuable use cases that leverage LLM technology will do so without anything that resembles RAG. </p> <p>In the last year, I've advised several innovative companies on their GenAI strategy, and I've implemented applications of this technology for use cases across industries. A subset of GenAI use cases deal with knowledge bases and other large bodies of content. With a nod to the legendary book by Martin Kleppmann, I call these knowledge-intensive applications. </p> <p>I want to provide a set of alternatives to the RAG Chatbot that product and technology leaders can reach to when conceptualizing knowledge-intensive applications. In this article, I'll describe when a RAG chatbot is useful and lay out four other application archetypes that are often more appropriate for a given use case.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-1-the-rag-chatbot","title":"Archetype 1: The RAG Chatbot","text":"<p>The idea, familiar to many practitioners, is to answer user questions by using retrieved knowledge from a knowledge base. Variations and enhancements to RAG, such as reranking, temporal (or other) filtering, and content evaluation fall under this archetype.</p> <p>The distinguishing feature is using a language model at the end of a set of operations (e.g., filtering, retrieval, summarization) to provide a user response in natural language.</p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful","title":"When is it useful?","text":"<p>I only reach for the RAG Chatbot archetype when the interface needs to be a chatbot. Examples include a sales/customer service agent or a therapist bot. The conversational interface is crucial, and the augmented knowledge enhances effectiveness.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#the-problem-with-rag","title":"The Problem with RAG","text":"<p>When I implemented my first RAG application, I was impressed with the concept. It seems unstoppable - if an LLM has all the context relevant for a user's query, it can synthesize that knowledge to provide useful information. As a developer, I tested the proof-of-concept application I had developed by searching for some interesting fact in the content and formulating it as a question. Voil\u00e1! The content was retrieved and the LLM dutifully regurgitated as an answer. </p> <p>The problem came when I tested questions that real users were interested in. These questions required synthesis, aggregation, calculation, multi-hop reasoning. With enough tweaking and \"agentic\" application logic, I could get the system to effectively answer some of these classes of questions. But there seemed to be an infinite set of question classes. </p> <p>At the end of the day, real users aren't interested in \"control-F on steroids\", and business execs aren't interested in a productivity tool for their analysts and operators. What everyone really wants is scalable intelligence that unlocks use cases that wouldn't be possible without it. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-2-structured-generation","title":"Archetype 2: Structured Generation","text":"<p>For many use cases, unstructured content is more useful when it's structured. If I have ten thousand PDFs with a similar structure, I can't do much analysis. What I want is an Excel table with ten thousand rows.</p> <p>Structured Generation applications leverage an LLM to perform a basic task at scale. Business leaders know what these tasks would be - the same thing they would do if they had a roman legion of interns! They would create a well-defined task that requires basic (albeit limited) intelligence, repeat it all summer storing the result in a digestible format, and then hand it over experienced analysts leverage this new competitive dataset as they saw fit. </p> <p>The distinguishing feature for this application archetype is applying an LLM for element-wise structural generation over a corpus of documents. This operation can incur a significant upfront cost but eliminates the need for LLM calls during user analysis of the resulting structured data.</p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_1","title":"When is it useful?","text":"<p>Data is valuable when it's usable. Many end users simply prefer structured datasets for hands-on analysis rather than interacting through a conversational interface. For instance, in private equity deal sourcing, a well-structured Excel spreadsheet is preferred to almost any format short of a deal memo.</p> <p>The same goes for generated datasets used by other software systems. An example here is literature mining for protein sequence-to-behavior mappings in chemical research. Protein engineering tools can use an augmented database of mined data much better than they can use a chatbot!</p> <p>To identify use cases, a good place to start is by identifying tasks that require some degree of intelligence, such that an LLM can perform the task, but not basic NLP or data mining. Basic literacy in the interpretation of financial statements is a good example. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#upfront-costs","title":"Upfront costs","text":"<p>Structured Generation will differ from RAG Chatbots in terms of the cost structure as well. Whereas RAG chatbots have very low upfront costs, both in terms of development time to POC and LLM calls, structured generation applications can require significant costs. Compute alone could cost several thousands of dollars if the document corpus is large. These costs will be split between basic document OCR and conversion and LLM calls. Structured generation applications also tend to require more initial development in order to create a processing pipeline and define the optimal structure, but tools like LlamaParse and Instructor help streamline it significantly. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-3-the-generative-query-engine","title":"Archetype 3: The Generative Query Engine","text":"<p>Sometimes, the owner of a data system might want to provide a natural language interface for querying, analyzing, and visualizing the data. In this case, the user asks a question, the LLM generates a query in the query language of the data system, and the data is either provided back to the user or visualized in a chart.</p> <p></p> <p>This architecture leverages the interpolative nature of LLMs. With some minor fine-tuning and few-shot prompting (i.e., providing example queries in the prompt), an LLM can effectively interpret the specification of a query language and generalize its knowledge to the entire range of possible queries. In other words, LLMs are naturally suited to be query generators. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_2","title":"When is it useful?","text":"<p>Many data systems, like the Bloomberg terminal, have a bifurcated userbase. They have a few power users who can write complex queries and generate high-quality charts, and a large number of casual users who use only basic functionality. A natural language interface lets casual users get more value from the product without extensive training. </p> <p>For internal business intelligence dashboards, a natural language query engine puts the tool in the hands of executives, who actually use the insights, rather than having them rely on a layer of business analysts who serve as intermediaries. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-4-structured-generation-with-generative-query-engine","title":"Archetype 4: Structured Generation with Generative Query Engine","text":"<p>By combining archetypes 2 and 3, we can achieve natural language intelligence on a large set of documents. The documents are first processed for structure (archetype 2) and stored in an appropriate database. Then a query engine generates and runs queries and visualizations based on user input.</p> <p></p> <p>On the surface, this archetype looks like RAG! A user asks questions and the system responds with insights based on content from a corpus of documents. However, this architecture is much more effective for many business use cases that require a degree of analysis, aggregation or calculation. More concretely, unless the answer is explicitly stated in the document content, a RAG-based application will not be able to answer quantitative (e.g., \"how many\") questions. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_3","title":"When is it useful?","text":"<p>As with archetype 1, it's important to question the value of providing a chat interface. If users are better served by having access to the data in structured form, archetype 2 might be a better solution. </p> <p>If the structured generation stores data in a database that the target user is not comfortable querying, it would be beneficial to provide a query engine. This could be a simple CSV or tables in an SQL database.</p> <p>Note</p> <p>In theory, although I've never done so myself, the structured generation could produce data in such a complex format that it wouldn't be practical for any human to query, like a complex graph database schema. I suspect that in the coming years, highly intelligent and reliable agents will handle knowledge by developing their own schema that is only practical for intelligent language models to query.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-5-document-editor-co-pilot","title":"Archetype 5: Document Editor Co-Pilot","text":"<p>A lot of technical writing is a slog of translating immense amounts of knowledge into a templated structure and ensuring its consistency throughout iterations and project advancements. Writing skills and technical knowledge being in short supply, these writers tend to be among the most valuable members of our society, and we force them to spend their time writing these documents - cancer researchers writing clinical trial protocols (100-150 pages), government officials writing an environmental impact statement (500-1000+ pages), and yes, even open source library developers writing documentation. </p> <p>Danger</p> <p>I haven't had the chance to develop a document editor co-pilot, so my architectural desciption is speculative</p> <p>As I imagine it, the architecture will include a generated knowledge base of facts, a (possibly configurable) template for the document structure, and a set of writer/editor agents that work alongside authors to write and refine the document. </p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_4","title":"When is it useful?","text":"<p>Productivity tools for technical writers are valuable, but only when they leave the writer in the driver's seat. Technical writers don't want auto-complete of the paragraph they are writing. They want an intelligent assistant to fill in parts of the template and help maintain consistency. For example, when the writer updates a set of assumptions or facts in one part of the document, the assistant should propose a set of changes to the rest of the document. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#conclusion","title":"Conclusion","text":"<p>It's not always a chatbot, and it's not always RAG. One of my favorite things about working with business leaders in this space is the moment after I've laid out this set of alternatives archetypes and the flood gates open for AI product ideas, coming from every part of the organization.</p> <p>Share on </p>"},{"location":"blog/2016/07/08/deep-learning-reading-list/","title":"Deep Learning Reading List","text":"<p>There is a lot written online about deep learning and AI. Ignoring the fluff and product-hype articles, you\u2019re left with a huge resource for learning the technical side of the field. I won\u2019t claim to have read everything out there, but here are some of the most useful things that I have found. The purpose of this list is not to be a definitive curriculum, but to recommend some of the high-quality educational material that\u2019s out there. If there is an article that you think belongs on this list send me an email. </p> <p>Edit (June 2024)</p> <p>This post is from 2016! I've revamped this blog, but kept a few posts from that era, which was a period of intense technical learning and exploration that I look back on fondly.</p> <p>Education sharing in specialist fields is really cool and we should consider ourselves lucky to have it. My career so far has been based on the fact that anyone with math literacy can self-educate their way through tech.</p> <p>Adrej Karpathy's blog especially his articles on recurrent neural networks and reinforcement learning. He's a fantastic communicator and educator and he also wrote the classic example of a RNN.</p> <p>Chris Olah's blog, his article on LSTM explains the difficult to explain.</p> <p>I use tensorflow, google's deep learning python package. It is well documented and if I can teach it to myself in my spare time so can you.</p> <p>Share on </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/","title":"A tour through tensorflow with financial data","text":"<p>I present several models ranging in complexity from simple regression to LSTM and policy networks. The series can be used as an educational resource for tensorflow or deep learning, a reference aid, or a source of ideas on how to apply deep learning techniques to problems that are outside of the usual deep learning fields (vision, natural language).</p> <p>Edit (June 2024)</p> <p>This post is from 2016! I've revamped this blog, but kept a few posts from that era, which was a period of intense technical learning and exploration that I look back on fondly.</p> <p>Not all of the examples will work. Some of them are far to simple to even be considered viable trading strategies and are only presented for educational purposes. Others, in the notebook form I present, have not been trained for the proper amount of time. Perhaps with a bit of rented GPU time they will be more promising and I leave that as an excercise for the reader. Hopefully this project inspires some to try using deep learning techniques for some more interesting problems. Contact me if interested in learning more or if you have suggestions for additions or improvements. </p> <p>The algorithms increase in complexity and introduce new concepts as they progress:</p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#simple-regression-notebook","title":"Simple Regression (notebook)","text":"<p>Here we regress the prices from the last 100 days to the next day's price, training W and b in the equation <code>y = Wx + b</code> where y is the next day's price, x is a vector of dimension 100, W is a 100x1 matrix and b is a 1x1 matrix. We run the gradient descent algorithm to minimize the mean squared error of the predicted price and the actual next day price. Congratulations, you passed highschool stats. But hopefully this simple and naive example helps demonstrate the idea of a tensor graph, as well as showing a great example of extreme overfitting. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#simple-regression-on-multiple-symbols-notebook","title":"Simple Regression on Multiple Symbols (notebook)","text":"<p>Things get a little more interesting as soon as we introduce more than one symbol. What is the best way to model our eventual investment strategy? We start to realize that our model only vaguely implies a policy (investment actions) by predicting the actual movement in price. The implied policy is simple: buy if the the predicted price movement is positive, sell if it is negative. But that doesnt sound realistic at all. How much do we buy? And will optimizing this, even if we are very careful to avoid overfitting, even produce results that allign with our goals? We havent actaully defined our goals explicitly, but for those who are not familiar with investment metrics, common goals include: + maximize risk adjusted return (like the Sharpe ratio) + consistency of returns over time + low market exposure + long/short equity </p> <p>If markets were easy to figure out and we could accurately predict the next day's return then it wouldn't matter. Our implied policy would fit with some goals (not long/short equity though) and the strategy would be viable. The reality is that our model cannot accurately predict this, nor will our strategy ever be perfect. Our best case scenario is always just winning slightly more than losing. When operating on these margins it is much more important that we consider the policy explicitly, thus moving to 'Policy Based' deep learning. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#policy-gradient-training-notebook","title":"Policy Gradient Training (notebook)","text":"<p>Our policy will remain simple. We will chose a position, long/neutral/short, for each symbol in our portfolio. But now, instead of letting our estimation of the future return inform our decision, we train our network to choose the best position. Thus, instead of having an implied policy, it is explicit and trained directly.   Even thought the policy is simple in this case, training it is a bit more involved. I did my best to interpret Andrej Karpathy's excelent article on Reinforcement Learning when writing this code. It might be worth reading his explanation, but I'll do my best to summarize what I did.</p> <p>We update our regression engine so that its output, y, is a vector in dimension [batch_size, number_positions x number_symbols] (a long, short, neutral bucket for each symbol). </p> <p>For each symbol in our portfolio, we sample the probability distribution of our three position buckets to get our policy decision (a position long/short/neutral), we multiply our decision (element of {-1,0,1}) by the target value to get a daily return for the symbol. Then we add that value for all the symbols to get a full daily return. We can also get other metrics like the total return and sharpe ratio since we actually are feeding this through as a batch (more on that later). As Karpathy points out, we are only interested in the gradients of the positions we sampled, so we select the appropriate columns from the output and combine them into a new tensor. </p> <p>This part of the code was a bit tricky for several reasons. First off, we have a loop through and isolate each symbol since I need one position per symbol. I also am using multinomial probability distributions, so I need to take a softmax of those values. Softmax pushes values so that they sum to 1, and therefore can represent a probability distribution. In pseudocode: <code>softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))</code>. </p> <pre><code>for i in range(len(symbol_list)):\n    symbol_probs = y[:,i*num_positions:(i+1)*num_positions]\n    symbol_probs_softmax = tf.nn.softmax(symbol_probs)\n</code></pre> <p>Next, we sample that probability distribution. Even though the code is a nice one-liner due to tensorflow's multinomial function, the function is NOT DIFFERENTIABLE, meaning that we will not be able to \"move through\" this step durring back propogation. We calcultate the position vector simply by subtracting 1 from the column indices that we got from the sample so that we get and element of {-1,0,1}.</p> <pre><code>pos = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE from before\n    # ... \n    sample = tf.multinomial(tf.log(symbol_probs_softmax), 1)\n    pos[i] = tf.reshape(sample, [-1]) - 1   # choose(-1,0,1)\n</code></pre> <p>Then we multiply that position by the target (future return) for each day. This gives us our return. It already looks like a cost function but remember that it's not differentiable. </p> <pre><code>symbol_returns = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE and SAMPLE from before\n    # ...\n    symbol_returns[i] = tf.mul(tf.cast(pos[i], float32),  y_[:,i])\n</code></pre> <p>Finally, we isolate the relevant column (the one we chose in our sample) from our probability distribution. The idea isn't very difficult but the code was a bit tough, remember that we are dealing with a whole batch of outputs at a time. This step NEEDS to be differentiable since we will use this tensor to compute our gradients. Unfortunately tensorflow is still developing a function that does it by itself, here is the discussion. I actually think that my solution is the best and I suggest it in the discussion, but I'm really not an expert at efficient computation. If anyone thinks of a more efficient solution or if tensorflow finishes theirs please let me know. </p> <pre><code>relevant_target_column = {}\nfor i in range(len(symbol_list)):\n    # ...\n    # ...\n    sample_mask = tf.reshape(tf.one_hot(sample, 3), [-1,3])\n    relevant_target_column[i] = tf.reduce_sum(symbol_probs_softmax * sample_mask,1)\n</code></pre> <p>So here is all of that together:</p> <pre><code># loop through symbols, taking the buckets for one symbol at a time\npos = {}\nsymbol_returns = {}\nrelevant_target_column = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE the buckets relevant to the symbol and get a softmax as well\n    symbol_probs = y[:,i*num_positions:(i+1)*num_positions]\n    symbol_probs_softmax = tf.nn.softmax(symbol_probs)\n    # SAMPLE probability to chose our policy's action\n    sample = tf.multinomial(tf.log(symbol_probs_softmax), 1)\n    pos[i] = tf.reshape(sample, [-1]) - 1   # choose(-1,0,1)\n    # GET RETURNS by multiplying the policy (position taken) by the target return (y_) for that day\n    symbol_returns[i] = tf.mul(tf.cast(pos[i], float32),  y_[:,i])\n    # isolate the output probability the selected policy (for use in calculating gradient)\n    sample_mask = tf.reshape(tf.one_hot(sample, 3), [-1,3])\n    relevant_target_column[i] = tf.reduce_sum(symbol_probs_softmax * sample_mask,1)\n</code></pre> <p>So now we have a tensor with the regression's probability for the chosen (sampled) action for each symbol and each day. We also have a few performance metrics like daily and total return to choose from, but they're not differentiable because we sampled the probability so we cant just \"gradient descent maximize\" the profit...unfortunately. Instead, we find the sigmoid cross entropy (a sort of distance function) between the first table (the probabilities we chose/sampled) and an all-ones tensor of the same shape. We get a table of cross entropies of the same size (number of symbols by batch size) This is basically equivalent to saying, how do I do MORE of what I'm already doing, for every decision that I made.  <pre><code>training_target_cols = tf.concat(1, [tf.reshape(t, [-1,1]) for t in relevant_target_column.values()])\nones = tf.ones_like(training_target_cols)\ngradient = tf.nn.sigmoid_cross_entropy_with_logits(training_target_cols, ones)\n</code></pre></p> <p>Now we dont necessarilly want MORE of what we're doing, but the opposite of it is definitely LESS of it, which is useful. We multiply that tensor by our fitness function (the daily or aggregate return) and we use the gradient descent optimizer to minimize the cost. So you see? If the fitness function is negative, it will train the weights of the regression to NOT do what it just did. Its a pretty cool idea and it can be applied to a lot of problems that are much more interesting. I give some examples in the notebook about which different fitness functions you can apply which I think is better explained by seeing it. Here is that code: <pre><code>cost = tf.mul(gradient , returns)        #returns are some reshaped version of symbol_returns (from above)\noptimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n</code></pre></p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#stochastic-gradient-descent-notebook","title":"Stochastic Gradient Descent (notebook)","text":"<p>As you saw in the notebook, the policy gradient doesnt train very well when we are grading it on the return over the entire dataset, but it trains very well when it uses each day's return or the position on each symbol every day. This makes sense, if we just take the total return over several years and its slightly positive then we tell our machine to do more of that. That will do almost nothing since so many of those decisions were actually losing money. The problem, as we have it set up now, needs to be broken down into smaller units. Fortunately there is some mathematical proof that this is legal and even faster. Score! </p> <p>Stochastic Gradient Descent is basically just breaking your data into smaller batches and doing gradient descent on each one. It will have slightly less accurate gradients WRT to the entire dataset's cost function, but since you are able to iterate faster with smaller batches you can run way more of them. There might even be more advantages to SGD that I'm not even mentioning, so you can read the wikipedia or hear Andrew Ng talk about it. Or just use it since it works and its faster. If you're going on a wikipedia learning binge you might as well also learn about Momentum and Adagrad which are just variations. The latter two only really useful for people doing much bigger projects. If you are working on a huge project and your twobucksanhour AWS GPU instance is too slow then you should definitely be using them (and not be reading this introductory tutorial). At the higher level, the problem of tuning the optimizer and overall efficiency have been thoroughly researched. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#multi-sampling-notebook","title":"Multi Sampling (notebook)","text":"<p>Since we are sampling the policy, we can sample repeatedly in order to compute better. Karpathy's article summarizes the math behind this nicely and this paper is worth reading. The concept is intuitive and simple, but getting the math to work out and the tensor graph in order is very involved. One realizes that a mastery of numpy and a solid understanding of linear algebra are very important to tensorflow once the problems get...deeper, I guess is the word.</p> <p>Multi sampling adds a useful computational kick that lets the network train much more efficiently. The results are already impressive. Using batches of less than 75 days and only training on the total return over that timeframe, we are able to \"overfit\" our network. Keep in mind that all we are doing is telling the network to do more of what it is doing when it does well, and less when it does poorly! Sure, we are still far away from having anything worthwhile out of sample, but that is because we are still using linear regression. </p> <p>By now you are probably either wondering does this guy even know what deep learning is? I havent seen a single neural network! or you completely forgot we were still using the same linear regression that 16 year olds learn in math class. Well, we'll get to neural networks next but I wanted to talk about other things before neural networks to show how much tensorflow can be used for before neural networks even get mentioned, and to show how much important math exists in deep learning that has nothing to do with neural networks. Tensorflow makes neural nets so easy that you barely even notice that they're part of the picture and its definitely not worth getting bogged down by their math if you dont have a solid understanding of the math behind cross entropy, policy gradients and the like. They probably even distract from where the true difficulty is. Maybe I'll try to get a regression to play pong so that everyone shuts about neural networks and starts talking about policy learning...</p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#neural-networks-notebook","title":"Neural Networks (notebook)","text":"<p>So we finanlly get to it. Here's the same thing with a neural network. Its the simplest kind of net that there is but it is still very powerful. Way more powerful that our puny regression ever was becuase it has nonlinearities (RELU layers) between the other layers (which are basically just regressions by themselves). A product of linear regressions is still obviously linear<sup>1</sup>, but if we put a nonlinearity between the layers, then our net can do much more. Thats what gets people excited about neural networks: becuase they can hold enourmous amounts of information when trained well. Fortunatly, we just learned a bunch of cool ways to train them in steps 1-5. Now putting in the network is very easy. I really changed nothing except the Variable and the equation really is basically still y = Wx + b except non-linear W. </p> <p>The reason I introduced the networks so late is becuase they can be a bit difficult to tune. Chosing the right size of your network and the right training step can be difficult and sometimes it is helpful to start out simple until you have all the bells and whistles in place. </p> <p>In steps 3-5, we spent a lot of time figuring out tricks to do with the training step, which is a widely researched area at the moment and is probably more relevant to algorithmic trading that anything else. Now we are starting to demonstrate some of the techniques used in the prediction engine (regression before, neural network now). I believe this is a much more researched area and TensorFlow is better equiped for it. Many people describe the types of neural networks that we will learn as cells or legos. You dont need to think that much about how it works as long as you know what it does. If you noticed, thats what I did with the neural network. There is a lot more to learn and its worth learning, but when you're actually building with it, you dont think about RELU layers as much as input/output and a black box in the middle. Or at least I do...there are a bunch of people in image processing who look inside networks and do very cool things. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#regularization-and-modularization-notebook","title":"Regularization and Modularization (notebook)","text":"<p>From Wikipedia, \"regularization is the introduction of additional information in order to solve an ill-posed problem or to prevent overfitting.\" For our purposes, it is any technique used to reduce overfitting. One of the simplest yet most important examples is early stopping, that is, ending gradient descent before there is no significant gain in evaluation performance. The idea is that once the model stops improving, it will start to overfit the training data. Most overfitting can be avoided with just this technique. </p> <p>We have more overfitting problems. Financial data is very noisey with faint signals that are very complex. Markets are zero sum and there are already an enormous number of very smart people getting paid a very large amount of money to develop trading strategies. There are many different patterns that can be learned but we can assume that all of the simple ones have been found (and therefore traded out of the market). Also, the market structure and properties change with time so even if we found a great pattern for the past five years, it might not work a month from now. </p> <p>We therefore need even more strategies to prevent overfitting. One strategy is called L2 regularization. Basically, we punish a network for having very large weights by adding the L2 norm of the weights, \u00bd(norm(W)^2_2), times a constant B (to determine how much we want to regularize). It allows us to control the level of model complexity.</p> <p>Another method is called dropout regularization, a technique developed by Geoffrey Hinton. Overfitting is avoided by randomly ommitting half of the neurons during each training iteration. It sounds kooky but the idea is that it avoids the co-adaptation of features in the neural network, so that recognizing a certain set of features does not imply another set, as is the case in many overtrained nets. This way, each neuron will learn to detect a feature that is generally helpful. In validation and normal use, the neurons are not dropped so as to use all informaation. The technique makes the model more robust, avoids overfitting, and essentially turns your network into an ensemble that reaches consensus. Tensorflow's dropout layer includes the scaling required to safely ensemble when it comes time for validation. </p> <p>Since we want the same model with two different configurations--one with dropout active and one without--we will now pursue our long-overdue duty of refactoring our code. Although the code is presented in a single notebook, keep in mind that what I am essentially doing is modularizing. I won't claim that I am the most organized with my code, but I tried to keep it consistent with the best practices that I have observed others using with their open projects. Moving forward we will keep this structure becuase it is clearly superior to the organization that I had used before. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#lstm-notebook","title":"LSTM (notebook)","text":"<p>My favorite neural network, and a true stepping stone into real deep learning is the long short-term memory network, or LSTM. Colah wrote an incredibly clear explanation of LSTM and there is really no substitute to reading his post. To describe the setup as briefly as possible, you input the data one timestep at a time to the LSTM cell. And each timestep the cell not only recieves the new input, but it recieves the last timestep's output and what is called the cell state, a vector that carries information about what happened in the past. Within the cell you have trained gates (basically small neural nets) that decide, based on the three inputs, what to forget from the past cell state, what to remember (or add) to the new state, and what to output this timestep. It is a very powerful tool and fascinating in how effective it is. </p> <p>I am now pretty far into this series and I have a pretty good idea of where it will go from here. These are the problems that I must tackle in no particular order: * new policies such as     + long/short equality amongs two symbols and more     + spread trading (if that is different from above)     + minimize correlation/ new risk meaesure that is appropriate for large number of symbols * migrating to AWS and using GPU computing power * ensebling large numbers of strategies that are generated with the same code     + policy grads find local maxima so no reason not to use that to my advantage * testing suite to be able to test if the strategies are viable objectively * convolution nets, especially among larger groups of symbols we can expect that some patterns are fractal * turning it into a more formal project or web app</p> <p>And of course we can start moving to other sources of data: * text * scraping * games</p> <p>Stay tuned for some articles that I will write about the algorithms used here and a discussion of the difficulties of using these techniques for algorithmic trading developement.  </p> <p>1: I hope you didnt sleep through Linear Algebra class! I owe all my LA skills to Comrade Otto Bretscher of Colby College whose class I did sleep through but whose text book is worth its weight in gold.</p> <p>Share on </p>"},{"location":"services/","title":"Consulting Services - Applied Machine Learning and AI","text":"<p>I help companies apply AI and ML technology to their most pressing business problems. With a decade of experience as a machine learning engineer, I bring deep expertise and a commitment to high standards that ensures project success.</p> <p>My experience at a successful SaaS startup and a top consulting firm has given me a unique blend of technical and business skills. I pride myself on my ability to engage deeply on both sides of the equation, and this focus has won me a proven track record of addressing complex challenges for clients.</p>"},{"location":"services/#offerings","title":"Offerings","text":""},{"location":"services/#workshops-and-strategy","title":"Workshops and Strategy","text":""},{"location":"services/#ai-ideation-and-feasibility-workshop","title":"AI Ideation and Feasibility Workshop","text":"<p>Explore potential use cases and application areas for AI in your business, in terms of value and level-of-effort. </p>"},{"location":"services/#ai-roadmap-planning","title":"AI Roadmap Planning","text":"<p>Define a roadmap based on your company's AI strategic vision and current capabilities.</p>"},{"location":"services/#technical-coaching","title":"Technical Coaching","text":""},{"location":"services/#knowledge-intensive-application-design","title":"Knowledge-Intensive Application Design","text":"<p>Make the right product and architecture decisions to build GenAI applications that are reliable and valuable.</p>"},{"location":"services/#genai-rag-performance-strategy","title":"GenAI RAG Performance Strategy","text":"<p>Define a technical roadmap for improving the performance of GenAI applications by adopting a data-driven approach evaluation and measurement.</p>"},{"location":"services/#implementation-and-development","title":"Implementation and Development","text":""},{"location":"services/#genai-application-development","title":"GenAI Application Development","text":"<p>Ground an LLM agent in real-world data using the retrieval-augmented generation (RAG) architecture pattern.</p>"},{"location":"services/#ml-model-development","title":"ML Model Development","text":"<p>Develop machine learning models for forecasting, classification or regression. </p>"},{"location":"services/#geospatial-data-science","title":"Geospatial Data Science","text":"<p>Leverage geospatial data to gain insights and make data-driven decisions.</p>"},{"location":"services/genai_app_dev/","title":"GenAI RAG Application Development","text":""},{"location":"services/genai_app_dev/#objective","title":"Objective","text":"<p>Ground an LLM agent in real-world data using the retrieval-augmented generation (RAG) architecture pattern. </p>"},{"location":"services/genai_app_dev/#description","title":"Description","text":"<p>RAG applications come in many forms, from citation-enhanced chatbots to complex research assistants. They can be productivity tools for knowledge workers, business intelligence tools for decision makers, or product offerings for customers. </p> <p>Building on a strong foundation of ML application development, we've mastered the art of applying new LLM technology to knowledge-intensive applications. We are among the few who have gone beyond proof of concept technical demos, and learned the hard-won lessons that come from deploying these systems at scale. As we've learned, GenAI products become truly valuable when they reach a threshold of reliability. Building a reliable application out of unreliable components is our specialty. </p>"},{"location":"services/genai_app_dev/#engagement-details","title":"Engagement Details","text":""},{"location":"services/genai_app_dev/#key-activities","title":"Key Activities","text":"<ul> <li>Discovery: We'll work with you to assess data sources, define the application requirements, and align on solution architecture and design.</li> <li>Implementation: We'll rapidly develop and deploy an application, and continue to iteratively improve based on performance and feedback.</li> <li>Enablement: With comprehensive documentation and operational guides, we'll hand over the system ownership to your team, enabling them to operate and extend it.</li> </ul>"},{"location":"services/genai_app_dev/#deliverables","title":"Deliverables","text":"<ul> <li>Full stack GenAI application and source code</li> <li>Performance evaluation system for confidently estimating the reliability of the system</li> <li>Operational documentation and handover</li> </ul>"},{"location":"services/genai_app_dev/#highlights","title":"Highlights","text":"<ul> <li>Cloud agnostic</li> <li>LLM provider agnostic</li> </ul>"},{"location":"services/genai_rag_performance_strat/","title":"GenAI knowledge base performance strategy","text":""},{"location":"services/genai_rag_performance_strat/#objective","title":"Objective","text":"<p>Define a technical roadmap for improving the performance of knowledge base GenAI applications by adopting a data-driven approach evaluation and measurement. </p>"},{"location":"services/genai_rag_performance_strat/#description","title":"Description","text":"<p>Reliability is key to the value proposition of RAG-based application products. However, these applications are often plagued in production by quality problems such as hallucinations and other sub-optimal responses. By adopting a data-driven approach to evaluation, observability and development priorities, technical teams can build reliable systems out of unreliable components. </p> <p>With a decade of experience developing classic ML applications, and significant hands-on experience with GenAI applications in the last two years, Liam is a valuable advisor in addressing this long tail of necessary improvements for your GenAI knowledge base system. He takes a practical approach, defining a custom roadmap for your development team that prioritizes quick wins while laying the foundation for sophisticated domain-specific solutions. </p> <p>The discussions will also cover the best practices and tools for knowledge system development that top teams have adopted to optimize development velocity and shorten iteration cycles. With code examples and detailed diagrams, I define a standard of excellence for this new type of system that results in a significant improvement in response quality that end users experience. </p>"},{"location":"services/genai_rag_performance_strat/#engagement-details","title":"Engagement Details","text":""},{"location":"services/genai_rag_performance_strat/#key-activities","title":"Key Activities","text":"<p>Workshops: Through a series of discovery workshops, We'll review your current application, RAG approaches, evaluation systems and development processes. We'll describe best practices for the development team to adopt. </p> <p>Design: Based on your input, we'll design additional system components, processes and logical application flows for improving your GenAI system performance. </p> <p>Roadmap: Following our design collaboration, we'll summarize the current state of your GenAI application, identify the top priority features and initiatives to implement, and craft an implementation plan.</p>"},{"location":"services/genai_rag_performance_strat/#deliverables","title":"Deliverables","text":"<ul> <li>GenAI knowledge base performance workshops</li> <li>Custom roadmap and implementation plan for improving your GenAI knowledge base performance</li> <li>Summary of best practices and tools applicable to your application All diagrams and documentation developed during the engagement including source code, scripts, templates, and other technical artifacts</li> </ul>"},{"location":"services/technical_expertise/","title":"Technical expertise","text":""},{"location":"services/technical_expertise/#technical-expertise","title":"Technical Expertise","text":""},{"location":"services/technical_expertise/#genai-application-development-ai-engineering","title":"GenAI Application Development (AI Engineering)","text":"<p>\u201cAI Engineering\u201d is a term being used for the new domain around applying LLMs (open source / from providers) to develop the new application types that the technology unlocks - knowledge systems, content generation, workflow automation, semantic parsing and querying, and others.</p> <p>BCG invested an enormous amount of capacity in 2023 in order to get its staff and leadership hands-on experience in this area. I had the opportunity to develop demos and lead the development of several applications:</p> <ul> <li>Consumer-facing knowledge assistant product at a global B2B information services provider</li> <li>GenAI-enabled network incident trend detection for a large telecommunications company</li> <li>Agricultural industry strategic knowledge assistant</li> <li>Natural language geospatial query generation and visualization application for agronomists</li> </ul>"},{"location":"services/technical_expertise/#machine-learning-application-development","title":"Machine Learning Application Development","text":"<p>Most of my career has been around the development of data-intensive applications that leverage Machine Learning. I have deep experience in ML modeling techniques as well as the practical details of ML pipelines and the deployment of these systems to cloud services for scalability and effective hosting.</p> <p>Some examples of projects in this area: * Application and computer vision pipeline for leading insurance company to automate estimations of coverage in force (i.e., potential liabilities) * Crop yield forecast for US corn and soy that beat the USDA in a historical backtest and live during the 2023 growing season * Designed and trained a Reinforcement Learning agent to optimize machine operations at a food processing plant, using model-based and model-free RNN-based architecture.</p>"},{"location":"services/technical_expertise/#geospatial-data-science","title":"Geospatial Data Science","text":"<p>As the founding tech lead of BCG\u2019s Center for Earth Intelligence, I had the opportunity to gain deep expertise in the field of geospatial data science. Using a scalable tech stack in python in order to achieve the highest modeling expressibility, we applied advanced ML and computer vision techniques to petabyte-scale data across continents. We took on use cases in insurance assessments, climate risk modeling, agronomy, B2B agriculture sales, and supply chain simulation.</p>"},{"location":"services/technical_expertise/#technical-strategy-for-ai-and-ml","title":"Technical Strategy for AI and ML","text":"<p>At BCG, I had several opportunities to leverage my deep practical technical expertise in an advisory role. My communication and critical thinking skills gave me a natural aptitude for this type of work and I found it highly stimulating. Some examples: * Facilitated tech workshops with C-suite technologists across industries to define 2-year GenAI roadmaps. * Conducted technical audits and strategic reviews of tech startups. * Defined operating model for team of 8 experts to deliver value in new domain (geospatial DS). * Enabled the development of a cross-functional data science team at a consumer goods company by defining roles and guiding re-skilling efforts. * Weighed in on the feasibility of unproven use cases in GenAI and ML.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2016/","title":"2016","text":""}]}