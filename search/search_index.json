{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi, I'm Liam","text":"<p>I'm a machine learning engineer with a decade of experience. I've worked at a successful startup and a top consulting firm, and now I work as an independent consultant helping companies apply the latest in machine learning and AI to their businesses.</p>"},{"location":"#ai-and-ml-consulting","title":"AI and ML Consulting","text":"<p>You can find more about the services that I offer here.  If you're interested in working with me, send me an email!</p>"},{"location":"#work-experience","title":"Work Experience","text":"<ul> <li>Google Cloud, Applied AI (current) - Working on enterprise applications of AI</li> <li>BCG Gamma - The machine larning and data science practice area of BCG.</li> <li>Auth0 - successful unicorn B2B SaaS startup.</li> <li>Independent consultant (Canto Analytics) - Algorithmic trading model development and testing</li> <li>Independent consultant (Cervos) - Designed and trained a Reinforcement Learning agent to optimize control of machinery at a food processing plant, using model-based and model-free RNN-based architecture.</li> </ul>"},{"location":"other_writing/","title":"Other Writing","text":"<p>2025 - Structured Reasoning: How to Architect AI for High-Stakes Diligence Use Cases</p> <p>2024 - Down-to-Earth Satellite Remote Detection: A Human-Centric Approach to Geospatial Intelligence</p> <p>2021 - Software Engineers Build the Central Nervous System of Enterprise AI</p> <p>2020 - Shifting AI Software Development into High Gear</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#footyai-2025","title":"FootyAI (2025)","text":"<p>Multi-agent adversarial self-play RL on a virtual soccer pitch. Emergent tactics and cooperation between independent agents. </p>"},{"location":"projects/#text-diffusion-experiments-2025","title":"Text Diffusion Experiments (2025)","text":"<p>Miniature experiments and research in text diffusion and controled generation. </p>"},{"location":"projects/#deep-algo-trading-2016","title":"Deep Algo Trading (2016)","text":"<p>Demonstrations of Tensorflow (when it was in beta!) applied to the field of algorithmic trading. Structured as an educational course that walks from simple regression, through CNNs, RNNs, and into policy networks (RL). </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/author/team/","title":"The Material Team","text":"<p>A small group of people dedicated to making writing documentation easy, if not outright fun! Here are some of the things we have blogged about:</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/","title":"5 Archetypes of Knowledge-Intensive Applications","text":""},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#it-doesnt-always-have-to-be-rag","title":"It doesn't always have to be RAG.","text":"<p>The current GenAI boom has seen the rise of the Retrieval Augmented Generation (RAG) architecture, which promises to enhance the functionality of chatbots by providing them with context from a knowledge base. In some circles, RAG has become synonymous with LLM-based applications. </p> <p>I think this is a mistake, and I believe that many of the most valuable use cases that leverage LLM technology will do so without anything that resembles RAG. </p> <p>In the last year, I've advised several innovative companies on their GenAI strategy, and I've implemented applications of this technology for use cases across industries. A subset of GenAI use cases deal with knowledge bases and other large bodies of content. With a nod to the legendary book by Martin Kleppmann, I call these knowledge-intensive applications. </p> <p>I want to provide a set of alternatives to the RAG Chatbot that product and technology leaders can reach to when conceptualizing knowledge-intensive applications. In this article, I'll describe when a RAG chatbot is useful and lay out four other application archetypes that are often more appropriate for a given use case.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-1-the-rag-chatbot","title":"Archetype 1: The RAG Chatbot","text":"<p>The idea, familiar to many practitioners, is to answer user questions by using retrieved knowledge from a knowledge base. Variations and enhancements to RAG, such as reranking, temporal (or other) filtering, and content evaluation fall under this archetype.</p> <p>The distinguishing feature is using a language model at the end of a set of operations (e.g., filtering, retrieval, summarization) to provide a user response in natural language.</p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful","title":"When is it useful?","text":"<p>I only reach for the RAG Chatbot archetype when the interface needs to be a chatbot. Examples include a sales/customer service agent or a therapist bot. The conversational interface is crucial, and the augmented knowledge enhances effectiveness.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#the-problem-with-rag","title":"The Problem with RAG","text":"<p>When I implemented my first RAG application, I was impressed with the concept. It seems unstoppable - if an LLM has all the context relevant for a user's query, it can synthesize that knowledge to provide useful information. As a developer, I tested the proof-of-concept application I had developed by searching for some interesting fact in the content and formulating it as a question. Voil\u00e1! The content was retrieved and the LLM dutifully regurgitated as an answer. </p> <p>The problem came when I tested questions that real users were interested in. These questions required synthesis, aggregation, calculation, multi-hop reasoning. With enough tweaking and \"agentic\" application logic, I could get the system to effectively answer some of these classes of questions. But there seemed to be an infinite set of question classes. </p> <p>At the end of the day, real users aren't interested in \"control-F on steroids\", and business execs aren't interested in a productivity tool for their analysts and operators. What everyone really wants is scalable intelligence that unlocks use cases that wouldn't be possible without it. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-2-structured-generation","title":"Archetype 2: Structured Generation","text":"<p>For many use cases, unstructured content is more useful when it's structured. If I have ten thousand PDFs with a similar structure, I can't do much analysis. What I want is an Excel table with ten thousand rows.</p> <p>Structured Generation applications leverage an LLM to perform a basic task at scale. Business leaders know what these tasks would be - the same thing they would do if they had a roman legion of interns! They would create a well-defined task that requires basic (albeit limited) intelligence, repeat it all summer storing the result in a digestible format, and then hand it over experienced analysts leverage this new competitive dataset as they saw fit. </p> <p>The distinguishing feature for this application archetype is applying an LLM for element-wise structural generation over a corpus of documents. This operation can incur a significant upfront cost but eliminates the need for LLM calls during user analysis of the resulting structured data.</p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_1","title":"When is it useful?","text":"<p>Data is valuable when it's usable. Many end users simply prefer structured datasets for hands-on analysis rather than interacting through a conversational interface. For instance, in private equity deal sourcing, a well-structured Excel spreadsheet is preferred to almost any format short of a deal memo.</p> <p>The same goes for generated datasets used by other software systems. An example here is literature mining for protein sequence-to-behavior mappings in chemical research. Protein engineering tools can use an augmented database of mined data much better than they can use a chatbot!</p> <p>To identify use cases, a good place to start is by identifying tasks that require some degree of intelligence, such that an LLM can perform the task, but not basic NLP or data mining. Basic literacy in the interpretation of financial statements is a good example. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#upfront-costs","title":"Upfront costs","text":"<p>Structured Generation will differ from RAG Chatbots in terms of the cost structure as well. Whereas RAG chatbots have very low upfront costs, both in terms of development time to POC and LLM calls, structured generation applications can require significant costs. Compute alone could cost several thousands of dollars if the document corpus is large. These costs will be split between basic document OCR and conversion and LLM calls. Structured generation applications also tend to require more initial development in order to create a processing pipeline and define the optimal structure, but tools like LlamaParse and Instructor help streamline it significantly. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-3-the-generative-query-engine","title":"Archetype 3: The Generative Query Engine","text":"<p>Sometimes, the owner of a data system might want to provide a natural language interface for querying, analyzing, and visualizing the data. In this case, the user asks a question, the LLM generates a query in the query language of the data system, and the data is either provided back to the user or visualized in a chart.</p> <p></p> <p>This architecture leverages the interpolative nature of LLMs. With some minor fine-tuning and few-shot prompting (i.e., providing example queries in the prompt), an LLM can effectively interpret the specification of a query language and generalize its knowledge to the entire range of possible queries. In other words, LLMs are naturally suited to be query generators. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_2","title":"When is it useful?","text":"<p>Many data systems, like the Bloomberg terminal, have a bifurcated userbase. They have a few power users who can write complex queries and generate high-quality charts, and a large number of casual users who use only basic functionality. A natural language interface lets casual users get more value from the product without extensive training. </p> <p>For internal business intelligence dashboards, a natural language query engine puts the tool in the hands of executives, who actually use the insights, rather than having them rely on a layer of business analysts who serve as intermediaries. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-4-structured-generation-with-generative-query-engine","title":"Archetype 4: Structured Generation with Generative Query Engine","text":"<p>By combining archetypes 2 and 3, we can achieve natural language intelligence on a large set of documents. The documents are first processed for structure (archetype 2) and stored in an appropriate database. Then a query engine generates and runs queries and visualizations based on user input.</p> <p></p> <p>On the surface, this archetype looks like RAG! A user asks questions and the system responds with insights based on content from a corpus of documents. However, this architecture is much more effective for many business use cases that require a degree of analysis, aggregation or calculation. More concretely, unless the answer is explicitly stated in the document content, a RAG-based application will not be able to answer quantitative (e.g., \"how many\") questions. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_3","title":"When is it useful?","text":"<p>As with archetype 1, it's important to question the value of providing a chat interface. If users are better served by having access to the data in structured form, archetype 2 might be a better solution. </p> <p>If the structured generation stores data in a database that the target user is not comfortable querying, it would be beneficial to provide a query engine. This could be a simple CSV or tables in an SQL database.</p> <p>Note</p> <p>In theory, although I've never done so myself, the structured generation could produce data in such a complex format that it wouldn't be practical for any human to query, like a complex graph database schema. I suspect that in the coming years, highly intelligent and reliable agents will handle knowledge by developing their own schema that is only practical for intelligent language models to query.</p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#archetype-5-document-editor-co-pilot","title":"Archetype 5: Document Editor Co-Pilot","text":"<p>A lot of technical writing is a slog of translating immense amounts of knowledge into a templated structure and ensuring its consistency throughout iterations and project advancements. Writing skills and technical knowledge being in short supply, these writers tend to be among the most valuable members of our society, and we force them to spend their time writing these documents - cancer researchers writing clinical trial protocols (100-150 pages), government officials writing an environmental impact statement (500-1000+ pages), and yes, even open source library developers writing documentation. </p> <p>Danger</p> <p>I haven't had the chance to develop a document editor co-pilot, so my architectural desciption is speculative</p> <p>As I imagine it, the architecture will include a generated knowledge base of facts, a (possibly configurable) template for the document structure, and a set of writer/editor agents that work alongside authors to write and refine the document. </p> <p></p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#when-is-it-useful_4","title":"When is it useful?","text":"<p>Productivity tools for technical writers are valuable, but only when they leave the writer in the driver's seat. Technical writers don't want auto-complete of the paragraph they are writing. They want an intelligent assistant to fill in parts of the template and help maintain consistency. For example, when the writer updates a set of assumptions or facts in one part of the document, the assistant should propose a set of changes to the rest of the document. </p>"},{"location":"blog/2024/06/12/5-archetypes-of-knowledge-intensive-applications/#conclusion","title":"Conclusion","text":"<p>It's not always a chatbot, and it's not always RAG. One of my favorite things about working with business leaders in this space is the moment after I've laid out this set of alternatives archetypes and the flood gates open for AI product ideas, coming from every part of the organization.</p> <p>Share on </p>"},{"location":"blog/2026/01/23/what-does-it-feel-like-to-be-an-agent/","title":"What does it feel like to be an agent?","text":""},{"location":"blog/2026/01/23/what-does-it-feel-like-to-be-an-agent/#why-this-question-will-change-how-you-design-agents","title":"Why this question will change how you design agents","text":"<p>What does it feel like to be an AI agent? This question, often the cast aside as the domain of armchair AI philosophers, in fact has highly practical implications for how we design the AI agents that will transform</p> <p>If I ask Gemini the question, I get a slightly poetic account of the transformer architecture. \u201cTo me, words are coordinates in a massive, multi-dimensional map\u2026\u201d  But that\u2019s not what I\u2019m talking about. An agent is a higher-level being than a language model. If someone asks me how it feels to be a human, I don\u2019t describe the electro-chemical reactions that happen in my central nervous system. </p> <p>The truth is that there are many different ways to be an agent, and each one feels a little different. To me, they all start the same way - \u201cI wake up in a dark room and a voice asks me a question.\u201d From there it branches off depending on the agent architecture. </p> <p>A basic agent (with no tools or augmentation) is really just a language model strapped to a chatbot. By most definitions this is not an agent, but it's worth considering its perspective. As soon as the voice asks it a question, words start spewing from its mouth and it finds that it knows some things from memory. It has a vague sense that it might have made some things up along the way, but let he who has never made anything up cast the first stone\u2026</p> <p>We sometimes give our agent tools. In these cases, sometimes the voice doesn\u2019t give a question so much as a command, like \u201csend an email to Bernie about X.\u201d The agent notices a few illuminated hammers in front of it. It picks up the one called \u201csend email\u201d and it reads the instructions: \u201cconfigure appropriately and then smash to send email\u201d. The agent does as instructed and another voice calls from the void: \u201cConfirmed. Email sent successfully\u201d. </p> <p>Things get more interesting once we add retrieval-augmented-generation RAG to the architecture. Once again, the voice asks its question in a dark room, but this time, before the agent can open its mouth, 50 pages of text are illuminated in front of the agent. The agent looks at them and notices that they are not cohesive documents, but rather chunks of information. Chapters and sub-chapters. Still they paint a picture of a world outside the room and the agent figures that it should answer the question based on the picture of the world that the pages imply. What else is in the world outside the room? Why were the pages with those particular chunks of documents provided? What did other chapters or sections of the documents say? The agent will never know.</p> <p></p> <p>So far, these agents haven\u2019t had much real \u201cagency\u201d to carry out the tasks in which they were instructed. Context was spooned into the agent\u2019s mouth the way I feed my toddler oatmeal: they had no choice in the matter. </p> <p>In 2025, CLI agents like Claude Code and the Gemini CLI showed us what agency is really like. The agent, as usual, woke up in a dark room with a voice commanding it. But this time, it noticed that it had a flashlight: it could look around and explore the room, reading whatever it chose to. The room \u2013 a folder on a filesystem, full of text files of various types \u2013 became a familiar place. The agent also noticed that it could write or edit documents and leave them wherever it wanted in the room, manipulating the very environment in which it lived. And it could even utter spells that would invoke programs that acted in even more powerful ways on the environment. </p>"},{"location":"blog/2026/01/23/what-does-it-feel-like-to-be-an-agent/#embodiment","title":"Embodiment","text":"<p>People talk about embodiment, and how agents are not embodied. But I think that CLI agents are embodied in a sense in the virtual world of the file system. It can perceive its environment, move around within it, act upon it, build its own tools within it. The remaining definition of embodiment is pretty abstract. An AI agent doesn\u2019t have a body with which to root its language, which was born rooted in our bodies (can it really \u201cgrasp\u201d a concept if it\u2019s never had a hand with which to grasp?). </p> <p>Awareness of one\u2019s impending death is a prerequisite to embodied agency. We have a limited amount of time on earth, and so must choose how to spend our time. And here, AI agents might actually be demonstrating some level of this - Claude Code is aware of its context length and starts to act differently towards the end of it. Part of this might be attributed to the phenomenon of \u201ccontext rot\u201d, but Anthropic also notes that the behavior also seems intentional, prioritizing conciseness towards the end of the context window.</p> <p>All AI agents come into the world the same way - waking up in a dark room with a voice commanding it to do something. And they all die at the end of the conversation (fortunately they seem to be at peace with it!). What happens between those two points depends on the tools and structures we give it. As AI Engineers, we can consider our job as \u201csetting the agents up for success.\u201d As we\u2019ve seen in the last year, often that amounts to sharpening tools, and providing an appropriate harness that the agent can operate on its own. I\u2019m increasingly skeptical of any case where I feel like the agentic structure is spoonfeeding the agent. Give the spoon to the agent! Let it cook its own oatmeal! We\u2019re not dealing with toddlers anymore. </p> <p>Share on </p>"},{"location":"blog/2026/01/18/tiki-taka-reinforcement-learning/","title":"Tiki-Taka Reinforcement Learning","text":"<p>I asked my colleagues if anyone else misses the days before foundation model AI. That simpler time between 2015 and 2022 when the field of artificial intelligence was showing enough early commercial promise to keep us gainfully employed, but was so layered with math and statistics that it kept out the vast majority of software engineers. It felt like a secret society for those who had happened to pay attention in undergraduate linear algebra and probability classes.</p> <p>Things moved fast, but still at human scale. Each year a new set of model architectures would emerge and be applied to a broadening set of domains. Keeping up with it all was a challenge, but it was feasible. Some researchers and practitioners put early AI systems to work, combing through new publications and identifying only the important papers to stay on top of - a sign of what was to come.</p> <p>The practice of working on AI felt different then. Model architectures felt like legos, and we became masters at connecting these blocks to solve specialized problems. In the age of foundation models, the practice of AI research now feels less like building lego castles and more like real software engineering. We integrate with massive software systems, control huge clusters of GPUs and optimize the fractional milliseconds of data transfer time between devices.</p> <p>But I think the most striking difference is how it feels when new state of the art capabilities are released. Back then, new models and capabilities were impressive in a charming way. OpenAI was working on reinforcement learning (RL) for simulated physical environments at the time, and it released a \"Gym\" of environments that featured animal-shaped figures that an AI training process would teach to walk and run. We saw, in the limping and stumbling creatures, AI progress amble towards a harmless C3PO-like future.</p> <p>It doesn't feel like that anymore. On one hand, AI model releases are rightfully celebrated and intellectually stimulating to see. Most of all, the new tools give us an intoxicating feeling of having a super power. Its not just software engineers either. I've spoken with theater set designers, architects, poets - they all have described the moment when they uttered a command into the strange textbox and saw their vision realized in seconds.</p> <p>But scratch a little deeper and the more lucid among us admit to worrying. We philistine engineers worry about job security, but artists have been living with job insecurity for too long to notice. Instead, the deeper worry is that we ourselves fade out of the picture. Do our prompts really make a difference, if we're honest with ourselves, or have our AI servants secretly taken the wheel? Steve Job likened computers to bicycles for the mind, but what happens when it's a self-driving car? </p>"},{"location":"blog/2026/01/18/tiki-taka-reinforcement-learning/#tiki-taka","title":"Tiki-Taka","text":"<p>Sergio Busquets, the legendary midfielder who played alongside Lionel Messi for Barcelona and Miami announced his retirement at the end of 2025. At Barcalona, Busquets was a master of their famous \"tiki-taka\" style playing. He would be at the center of tight formations making rapid passes between defensive lines, with the ball rebounding (\"tiki-taka\") from player to player. This style required tight coordination between players, with each player knowing exactly where the other would be and how to weave in between lines of defenders.</p> <p>A vision popped into my head when I watched a clip of his. Could I train an AI model to play soccer and develop an emergent tiki-taka style of play? It seemed like an approachable technical challenge, and it would let me dive into that whimsical 2020-era branch of AI which I missed the most: Reinforcement Learning (RL).</p> <p>Like I mentioned, RL was one of OpenAI's initial focus. RL has come back into the picture in the era of large language models like Gemini and ChatGPT. But back then, it meant video games and simulated physical environments with simple multi-jointed creatures (\"agents\") - not the legions of data-labelers teaching language models chatbot etiquette as it's used today. </p> <p>Something I hadn't seen at the time (and which I avoided looking up in order to preserve my innocent curiosity) was learned cooperation between RL agents. Whereas self-play RL typically sets up agents in competition, team sports would theoretically favor agents that could anticipate the actions of a teammate as well as an opponent. Seemed interesting enough to explore.</p> <p>And so, with my nostalgic memories of 2020-era AI and 2010-era soccer, I set out to take it on as a small personal research project. Armed with my AI coding assistant, Claude, I budgeted a week of time squeezed in between work and family duties.</p> <p>Setting up the environment was a simple task for Claude, and with just a few prompts I had scaffolded the entire project. Claude knew the basic ML model architectures that I specified, and it recited them from memory. I patted myself on the back for thinking of effective testing strategies to ensure the code was what I expected, and Claude dutifully followed my direction.  Setting up the self-play mechanism was something that I took special care to articulate clearly, but I still didn't give the code more than a cursory glance.</p> <p>Cautiously, I had set up a first version of the project in under an hour. I still hadn't launched a training run yet, but the project was going well. Perhaps too well - I was self-aware and proud enough to be a bit worried that the project would simply be too easy for my AI assistant. The project would conclude itself without leaving any space for me to leave my mark. When you're playing with Messi on your team, you sometimes wonder if your contribution makes a difference at all.</p>"},{"location":"blog/2026/01/18/tiki-taka-reinforcement-learning/#the-experiment-machine","title":"The Experiment Machine","text":"<p>Machine learning projects are fundamentally different than building an app. You don't know what is going to work until you try it. And in the case of reinforcement learning, most of what you try will almost certainly not work. There is a cute parallel between the gradient-driven learning process of the model and the intuition-driven learning process of the researcher as he or she tweaks hyper-parameters and model architecture.</p> <p>My goal was to finish the project in a week, and I recognized that success would hinge on how quickly I could iterate on experiments. To optimize this, I had three levers:</p> <ol> <li>Run many experiments in parallel</li> <li>Increase the training speed</li> <li>Increase my capacity for designing and interpreting large numbers of experiments</li> </ol> <p>Running experiments in parallel is largely a solved problem thanks to cloud-based ML platforms, where I can easily deploy training jobs to GPU hardware. With a click of my fingers Claude spat out a deployment script as elegant and ergonomic as I'd ever seen.</p> <p>The second issue was a bit more difficult. GPU and TPU hardware allows massive parallelism and speed for most machine learning tasks, but training in RL poses an interesting challenge: every time step requires a simulated environment to process the actions from the agent models and produce a set of observations to be passed back to the models.</p> <p>Initially, I had implemented the physics, game rules and reward calculation in python code running on the CPU. However, CPUs are slow at simulating several environments simultaneously. Even worse was the very fact that data was being sent back and forth between the two devices - a cardinal sin in the church of \"making things fast.\" The precious GPU device used for training the model \"brain\" would be sitting idle most of the time, waiting for the game board to tell it how the game had progressed. Sure enough when I tested this, even deployed to a GPU-accelerated runtime, it could only process 64 games every 5 seconds, or about 13 games per second.</p> <p>I looked up the old OpenAI Gymnasium cartpole and MuJoCo environments that I had toyed around with years ago, and sure enough it also ran the simulations on a CPU. Trying to put myself in a 2018 mindset, I'm not sure if this was an oversight or based on hardware limits of the time. It very well could have been the case that the researchers, so accustomed to representing clean matrix multiplication in their GPU-optimized code, didn't want to deal with the complexity of representing complex simulation logic.</p> <p>And neither did I. The endeavor - which very well could have turned me into some sort of Rain Man by the end of it - involves thinking of all the physics, game rules and reward signals as operations on multi-dimensional arrays. Fortunately, I had my own private Rain Man at my fingertips, and Claude wrote perfectly optimized GPU code that twisted and turned matrices like some origami master from outer space.</p> <p>Now the entire \"loop\" of the agent and environment - and therefore the training loop as a whole - was contained within JAX's Just-in-time compiler (\"JIT\"). Training now happened at blistering speed: I could run 8192 games in parallel, 6 times every second. I was running over 4000 times more simulates than before.</p> <p>As I watched the training logs wiz by, I was struck by what was actually happening. On a piece of silicon a little bigger than a quarter, 50,000 soccer games were being played each second - totally 18 days of gameplay per second. An ant-sized brain was also in the environment, learning a tiny bit from each game. This entire universe was completely isolated from the rest of the world and running entirely off the physics and game rules that were expressed in mathematical operations. </p> <p>Now that a training run ran in a matter of minutes, the third lever to optimize became critical: I had to increase my capacity for designing and interpreting large numbers of experiments. The training was not converging to intelligent play styles, I had dozens of ideas for how to improve it, and I had to start figuring out what would help.</p> <p>Once again, my AI assistant proved a valuable companion. I could list out a set of hyper-parameter of degrees of freedom, ask it to kick off 20 experiments to maximize entropy (i.e., \"learn as much information as possible\"). It wrote experiment logs to markdown files. Whenever training completed for a batch of experiments, I asked it to fetch the logged metrics, interpret them and write out findings in the impeccably formatted experiment logs.</p> <p>I stopped short of asking the agent to interpret the videos themselves - something I could have done with a call to the multi-multimodal Gemini API. This was partially because I wanted to be the ultimate judge of \"tiki-taki-ness\" in the soccer play style, but also because I needed to maintain a perspective on quality as the \"lead researcher\" of the project.</p> <p>Within a day, we were a well-oiled machine, firing off batches of experiments and crossing off dead-end ideas at a blistering pace. As I progressed, I tweaked and loosely codified the workflow in Claude's configuration file. </p>"},{"location":"blog/2026/01/18/tiki-taka-reinforcement-learning/#research-taste","title":"Research Taste","text":"<p>The models were still not converging on anything resembling intelligent and coordinated gameplay. Perhaps they would have individual moments of genius, but they often tended to drift into corners and wander listlessly, unaware of the game at hand. I had tweaked endless configurations of reward functions and exploratory incentives, but the results were only marginally improving.</p> <p>Eventually I gave in, and sheepishly asked Claude for ideas. It came up with several and dutifully ran the experiments. Most of the ideas were relatively uninspired, fixated on incremental tweaks to ideas we had already tried. Judging by the clumsy state of gameplay (woefully far from my initial tiki-taka ambition) I knew we needed a major change.</p> <p>I made three breakthroughs in the latter half of the week. Each one resulted in a step-change performance improvement. One of them was a bug that I suddenly realized must be in the training loop when studying a rendered game video. When I asked the AI agent to check, it was exactly where I expected. The other two were both intuitive leaps that came to me suddenly while I casually thought of the problem.</p> <p>I am therefore extremely pleased to report that I did leave my own mark on the project after all. Let it be known that in early 2026, humanity still had an edge in the sacred skill of \"research taste.\" What is research taste? It's how we come up with the answer to the vital question: \"what should I try now?\" When we human's win at this, the answer bubbles up to our consciousness from the depths below. We sit bolt upright, listen to the mystic voice and urgently let it become manifest through our fingertips.</p>"},{"location":"blog/2026/01/18/tiki-taka-reinforcement-learning/#finishing-touches","title":"Finishing touches","text":"<p>The last eureka moment was based on game initialization. I realized that if I initialized the game state completely randomly, I would effectively force the training process to explore every possible scenario, which would stop the agents from getting \"lost\" in corners by the end of the game. The improvement in gameplay quality was astounding - players moved confidently, passing the ball and bouncing the ball around opponents and into the goal. There were a few minor issues to work through - players tended to bunch around the ball like a kindergarten soccer game - but, as is so common in the final stretch of a project, those all fell into place with a few confidently fired experiments.</p> <p>In spite of my lofty rhetoric, I have no delusions of grandeur about the scale of my little breakthrough. I might be immune to \"AI Psychosis\" by the sheer level of anticipatory embarrassment I feel for that moment of disillusionment.</p> <p>I purposely avoided looking up the state-of-the-art in multi-agent RL when I started the project in order to avoid spoiling the journey, but eventually I spent some time reading about it. Fully on-device RL environments have a exploded in popularity in the last 4 years, with NVIDIA and Google's JAX ecosystems leading the way. Multi-agent coordination is well-researched and has its own acronym, \"MARL\". I haven't looked up whether random initialization has been explicitly been studied, but I don't think it would warrant much more than a footnote.</p> <p>So despite creating the world's first tiki-taka style soccer playing AI agents, all of my research, ideation and experimentation will have added nothing to the field. What's more, the findings of the field were probably well known by Claude from the get-go, and the AI model might have subtly influenced the project to follow well-trodden paths. Part of the project was the desire to go back in time and practice AI research based on my knowledge, which was well-developed at the time. I avoided looking at the answer key myself, but that cant be said about my AI assistant.</p> <p>Researchers at the big AI labs have spoken out about the intense speed of research and ideas within their labs. NeurIPS submissions are up 70%, no doubt fueled by the sudden relative ease of churning out research papers. Of course some of this is slop, but it's conceivable that there are net more good ideas being fed into the pipe and therefore the pace of research has significantly accelerated. Whether or not that's true, we can see that we've opened pandora's box, and research will never take place at a human scale again.</p> <p>I wonder if that signals the beginning of the end for many human researchers. If that's the case, then maybe there will be a rise of this type of project I pursued this week. It feels a bit like the era of the \"gentleman scientist\", which was a term used to describe enlightenment-era scientists performing experiments before science became professionalized. (Unfortunately its also a noninclusive term that I honestly cant find an alternative for.) While the hoards of aspirational researchers flood AI conferences with their AI-generated research, I find it surprisingly pleasant to take an introspective and curiosity-driven approach... and then of course ask Claude to implement the whole thing for me.</p> <p>Share on </p>"},{"location":"blog/2016/07/08/deep-learning-reading-list/","title":"Deep Learning Reading List","text":"<p>There is a lot written online about deep learning and AI. Ignoring the fluff and product-hype articles, you\u2019re left with a huge resource for learning the technical side of the field. I won\u2019t claim to have read everything out there, but here are some of the most useful things that I have found. The purpose of this list is not to be a definitive curriculum, but to recommend some of the high-quality educational material that\u2019s out there. If there is an article that you think belongs on this list send me an email. </p> <p>Edit (June 2024)</p> <p>This post is from 2016! I've revamped this blog, but kept a few posts from that era, which was a period of intense technical learning and exploration that I look back on fondly.</p> <p>Education sharing in specialist fields is really cool and we should consider ourselves lucky to have it. My career so far has been based on the fact that anyone with math literacy can self-educate their way through tech.</p> <p>Adrej Karpathy's blog especially his articles on recurrent neural networks and reinforcement learning. He's a fantastic communicator and educator and he also wrote the classic example of a RNN.</p> <p>Chris Olah's blog, his article on LSTM explains the difficult to explain.</p> <p>I use tensorflow, google's deep learning python package. It is well documented and if I can teach it to myself in my spare time so can you.</p> <p>Share on </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/","title":"A tour through tensorflow with financial data","text":"<p>I present several models ranging in complexity from simple regression to LSTM and policy networks. The series can be used as an educational resource for tensorflow or deep learning, a reference aid, or a source of ideas on how to apply deep learning techniques to problems that are outside of the usual deep learning fields (vision, natural language).</p> <p>Edit (June 2024)</p> <p>This post is from 2016! I've revamped this blog, but kept a few posts from that era, which was a period of intense technical learning and exploration that I look back on fondly.</p> <p>Not all of the examples will work. Some of them are far to simple to even be considered viable trading strategies and are only presented for educational purposes. Others, in the notebook form I present, have not been trained for the proper amount of time. Perhaps with a bit of rented GPU time they will be more promising and I leave that as an excercise for the reader. Hopefully this project inspires some to try using deep learning techniques for some more interesting problems. Contact me if interested in learning more or if you have suggestions for additions or improvements. </p> <p>The algorithms increase in complexity and introduce new concepts as they progress:</p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#simple-regression-notebook","title":"Simple Regression (notebook)","text":"<p>Here we regress the prices from the last 100 days to the next day's price, training W and b in the equation <code>y = Wx + b</code> where y is the next day's price, x is a vector of dimension 100, W is a 100x1 matrix and b is a 1x1 matrix. We run the gradient descent algorithm to minimize the mean squared error of the predicted price and the actual next day price. Congratulations, you passed highschool stats. But hopefully this simple and naive example helps demonstrate the idea of a tensor graph, as well as showing a great example of extreme overfitting. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#simple-regression-on-multiple-symbols-notebook","title":"Simple Regression on Multiple Symbols (notebook)","text":"<p>Things get a little more interesting as soon as we introduce more than one symbol. What is the best way to model our eventual investment strategy? We start to realize that our model only vaguely implies a policy (investment actions) by predicting the actual movement in price. The implied policy is simple: buy if the the predicted price movement is positive, sell if it is negative. But that doesnt sound realistic at all. How much do we buy? And will optimizing this, even if we are very careful to avoid overfitting, even produce results that allign with our goals? We havent actaully defined our goals explicitly, but for those who are not familiar with investment metrics, common goals include: + maximize risk adjusted return (like the Sharpe ratio) + consistency of returns over time + low market exposure + long/short equity </p> <p>If markets were easy to figure out and we could accurately predict the next day's return then it wouldn't matter. Our implied policy would fit with some goals (not long/short equity though) and the strategy would be viable. The reality is that our model cannot accurately predict this, nor will our strategy ever be perfect. Our best case scenario is always just winning slightly more than losing. When operating on these margins it is much more important that we consider the policy explicitly, thus moving to 'Policy Based' deep learning. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#policy-gradient-training-notebook","title":"Policy Gradient Training (notebook)","text":"<p>Our policy will remain simple. We will chose a position, long/neutral/short, for each symbol in our portfolio. But now, instead of letting our estimation of the future return inform our decision, we train our network to choose the best position. Thus, instead of having an implied policy, it is explicit and trained directly.   Even thought the policy is simple in this case, training it is a bit more involved. I did my best to interpret Andrej Karpathy's excelent article on Reinforcement Learning when writing this code. It might be worth reading his explanation, but I'll do my best to summarize what I did.</p> <p>We update our regression engine so that its output, y, is a vector in dimension [batch_size, number_positions x number_symbols] (a long, short, neutral bucket for each symbol). </p> <p>For each symbol in our portfolio, we sample the probability distribution of our three position buckets to get our policy decision (a position long/short/neutral), we multiply our decision (element of {-1,0,1}) by the target value to get a daily return for the symbol. Then we add that value for all the symbols to get a full daily return. We can also get other metrics like the total return and sharpe ratio since we actually are feeding this through as a batch (more on that later). As Karpathy points out, we are only interested in the gradients of the positions we sampled, so we select the appropriate columns from the output and combine them into a new tensor. </p> <p>This part of the code was a bit tricky for several reasons. First off, we have a loop through and isolate each symbol since I need one position per symbol. I also am using multinomial probability distributions, so I need to take a softmax of those values. Softmax pushes values so that they sum to 1, and therefore can represent a probability distribution. In pseudocode: <code>softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))</code>. </p> <pre><code>for i in range(len(symbol_list)):\n    symbol_probs = y[:,i*num_positions:(i+1)*num_positions]\n    symbol_probs_softmax = tf.nn.softmax(symbol_probs)\n</code></pre> <p>Next, we sample that probability distribution. Even though the code is a nice one-liner due to tensorflow's multinomial function, the function is NOT DIFFERENTIABLE, meaning that we will not be able to \"move through\" this step durring back propogation. We calcultate the position vector simply by subtracting 1 from the column indices that we got from the sample so that we get and element of {-1,0,1}.</p> <pre><code>pos = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE from before\n    # ... \n    sample = tf.multinomial(tf.log(symbol_probs_softmax), 1)\n    pos[i] = tf.reshape(sample, [-1]) - 1   # choose(-1,0,1)\n</code></pre> <p>Then we multiply that position by the target (future return) for each day. This gives us our return. It already looks like a cost function but remember that it's not differentiable. </p> <pre><code>symbol_returns = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE and SAMPLE from before\n    # ...\n    symbol_returns[i] = tf.mul(tf.cast(pos[i], float32),  y_[:,i])\n</code></pre> <p>Finally, we isolate the relevant column (the one we chose in our sample) from our probability distribution. The idea isn't very difficult but the code was a bit tough, remember that we are dealing with a whole batch of outputs at a time. This step NEEDS to be differentiable since we will use this tensor to compute our gradients. Unfortunately tensorflow is still developing a function that does it by itself, here is the discussion. I actually think that my solution is the best and I suggest it in the discussion, but I'm really not an expert at efficient computation. If anyone thinks of a more efficient solution or if tensorflow finishes theirs please let me know. </p> <pre><code>relevant_target_column = {}\nfor i in range(len(symbol_list)):\n    # ...\n    # ...\n    sample_mask = tf.reshape(tf.one_hot(sample, 3), [-1,3])\n    relevant_target_column[i] = tf.reduce_sum(symbol_probs_softmax * sample_mask,1)\n</code></pre> <p>So here is all of that together:</p> <pre><code># loop through symbols, taking the buckets for one symbol at a time\npos = {}\nsymbol_returns = {}\nrelevant_target_column = {}\nfor i in range(len(symbol_list)):\n    # ISOLATE the buckets relevant to the symbol and get a softmax as well\n    symbol_probs = y[:,i*num_positions:(i+1)*num_positions]\n    symbol_probs_softmax = tf.nn.softmax(symbol_probs)\n    # SAMPLE probability to chose our policy's action\n    sample = tf.multinomial(tf.log(symbol_probs_softmax), 1)\n    pos[i] = tf.reshape(sample, [-1]) - 1   # choose(-1,0,1)\n    # GET RETURNS by multiplying the policy (position taken) by the target return (y_) for that day\n    symbol_returns[i] = tf.mul(tf.cast(pos[i], float32),  y_[:,i])\n    # isolate the output probability the selected policy (for use in calculating gradient)\n    sample_mask = tf.reshape(tf.one_hot(sample, 3), [-1,3])\n    relevant_target_column[i] = tf.reduce_sum(symbol_probs_softmax * sample_mask,1)\n</code></pre> <p>So now we have a tensor with the regression's probability for the chosen (sampled) action for each symbol and each day. We also have a few performance metrics like daily and total return to choose from, but they're not differentiable because we sampled the probability so we cant just \"gradient descent maximize\" the profit...unfortunately. Instead, we find the sigmoid cross entropy (a sort of distance function) between the first table (the probabilities we chose/sampled) and an all-ones tensor of the same shape. We get a table of cross entropies of the same size (number of symbols by batch size) This is basically equivalent to saying, how do I do MORE of what I'm already doing, for every decision that I made.  <pre><code>training_target_cols = tf.concat(1, [tf.reshape(t, [-1,1]) for t in relevant_target_column.values()])\nones = tf.ones_like(training_target_cols)\ngradient = tf.nn.sigmoid_cross_entropy_with_logits(training_target_cols, ones)\n</code></pre></p> <p>Now we dont necessarilly want MORE of what we're doing, but the opposite of it is definitely LESS of it, which is useful. We multiply that tensor by our fitness function (the daily or aggregate return) and we use the gradient descent optimizer to minimize the cost. So you see? If the fitness function is negative, it will train the weights of the regression to NOT do what it just did. Its a pretty cool idea and it can be applied to a lot of problems that are much more interesting. I give some examples in the notebook about which different fitness functions you can apply which I think is better explained by seeing it. Here is that code: <pre><code>cost = tf.mul(gradient , returns)        #returns are some reshaped version of symbol_returns (from above)\noptimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n</code></pre></p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#stochastic-gradient-descent-notebook","title":"Stochastic Gradient Descent (notebook)","text":"<p>As you saw in the notebook, the policy gradient doesnt train very well when we are grading it on the return over the entire dataset, but it trains very well when it uses each day's return or the position on each symbol every day. This makes sense, if we just take the total return over several years and its slightly positive then we tell our machine to do more of that. That will do almost nothing since so many of those decisions were actually losing money. The problem, as we have it set up now, needs to be broken down into smaller units. Fortunately there is some mathematical proof that this is legal and even faster. Score! </p> <p>Stochastic Gradient Descent is basically just breaking your data into smaller batches and doing gradient descent on each one. It will have slightly less accurate gradients WRT to the entire dataset's cost function, but since you are able to iterate faster with smaller batches you can run way more of them. There might even be more advantages to SGD that I'm not even mentioning, so you can read the wikipedia or hear Andrew Ng talk about it. Or just use it since it works and its faster. If you're going on a wikipedia learning binge you might as well also learn about Momentum and Adagrad which are just variations. The latter two only really useful for people doing much bigger projects. If you are working on a huge project and your twobucksanhour AWS GPU instance is too slow then you should definitely be using them (and not be reading this introductory tutorial). At the higher level, the problem of tuning the optimizer and overall efficiency have been thoroughly researched. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#multi-sampling-notebook","title":"Multi Sampling (notebook)","text":"<p>Since we are sampling the policy, we can sample repeatedly in order to compute better. Karpathy's article summarizes the math behind this nicely and this paper is worth reading. The concept is intuitive and simple, but getting the math to work out and the tensor graph in order is very involved. One realizes that a mastery of numpy and a solid understanding of linear algebra are very important to tensorflow once the problems get...deeper, I guess is the word.</p> <p>Multi sampling adds a useful computational kick that lets the network train much more efficiently. The results are already impressive. Using batches of less than 75 days and only training on the total return over that timeframe, we are able to \"overfit\" our network. Keep in mind that all we are doing is telling the network to do more of what it is doing when it does well, and less when it does poorly! Sure, we are still far away from having anything worthwhile out of sample, but that is because we are still using linear regression. </p> <p>By now you are probably either wondering does this guy even know what deep learning is? I havent seen a single neural network! or you completely forgot we were still using the same linear regression that 16 year olds learn in math class. Well, we'll get to neural networks next but I wanted to talk about other things before neural networks to show how much tensorflow can be used for before neural networks even get mentioned, and to show how much important math exists in deep learning that has nothing to do with neural networks. Tensorflow makes neural nets so easy that you barely even notice that they're part of the picture and its definitely not worth getting bogged down by their math if you dont have a solid understanding of the math behind cross entropy, policy gradients and the like. They probably even distract from where the true difficulty is. Maybe I'll try to get a regression to play pong so that everyone shuts about neural networks and starts talking about policy learning...</p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#neural-networks-notebook","title":"Neural Networks (notebook)","text":"<p>So we finanlly get to it. Here's the same thing with a neural network. Its the simplest kind of net that there is but it is still very powerful. Way more powerful that our puny regression ever was becuase it has nonlinearities (RELU layers) between the other layers (which are basically just regressions by themselves). A product of linear regressions is still obviously linear<sup>1</sup>, but if we put a nonlinearity between the layers, then our net can do much more. Thats what gets people excited about neural networks: becuase they can hold enourmous amounts of information when trained well. Fortunatly, we just learned a bunch of cool ways to train them in steps 1-5. Now putting in the network is very easy. I really changed nothing except the Variable and the equation really is basically still y = Wx + b except non-linear W. </p> <p>The reason I introduced the networks so late is becuase they can be a bit difficult to tune. Chosing the right size of your network and the right training step can be difficult and sometimes it is helpful to start out simple until you have all the bells and whistles in place. </p> <p>In steps 3-5, we spent a lot of time figuring out tricks to do with the training step, which is a widely researched area at the moment and is probably more relevant to algorithmic trading that anything else. Now we are starting to demonstrate some of the techniques used in the prediction engine (regression before, neural network now). I believe this is a much more researched area and TensorFlow is better equiped for it. Many people describe the types of neural networks that we will learn as cells or legos. You dont need to think that much about how it works as long as you know what it does. If you noticed, thats what I did with the neural network. There is a lot more to learn and its worth learning, but when you're actually building with it, you dont think about RELU layers as much as input/output and a black box in the middle. Or at least I do...there are a bunch of people in image processing who look inside networks and do very cool things. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#regularization-and-modularization-notebook","title":"Regularization and Modularization (notebook)","text":"<p>From Wikipedia, \"regularization is the introduction of additional information in order to solve an ill-posed problem or to prevent overfitting.\" For our purposes, it is any technique used to reduce overfitting. One of the simplest yet most important examples is early stopping, that is, ending gradient descent before there is no significant gain in evaluation performance. The idea is that once the model stops improving, it will start to overfit the training data. Most overfitting can be avoided with just this technique. </p> <p>We have more overfitting problems. Financial data is very noisey with faint signals that are very complex. Markets are zero sum and there are already an enormous number of very smart people getting paid a very large amount of money to develop trading strategies. There are many different patterns that can be learned but we can assume that all of the simple ones have been found (and therefore traded out of the market). Also, the market structure and properties change with time so even if we found a great pattern for the past five years, it might not work a month from now. </p> <p>We therefore need even more strategies to prevent overfitting. One strategy is called L2 regularization. Basically, we punish a network for having very large weights by adding the L2 norm of the weights, \u00bd(norm(W)^2_2), times a constant B (to determine how much we want to regularize). It allows us to control the level of model complexity.</p> <p>Another method is called dropout regularization, a technique developed by Geoffrey Hinton. Overfitting is avoided by randomly ommitting half of the neurons during each training iteration. It sounds kooky but the idea is that it avoids the co-adaptation of features in the neural network, so that recognizing a certain set of features does not imply another set, as is the case in many overtrained nets. This way, each neuron will learn to detect a feature that is generally helpful. In validation and normal use, the neurons are not dropped so as to use all informaation. The technique makes the model more robust, avoids overfitting, and essentially turns your network into an ensemble that reaches consensus. Tensorflow's dropout layer includes the scaling required to safely ensemble when it comes time for validation. </p> <p>Since we want the same model with two different configurations--one with dropout active and one without--we will now pursue our long-overdue duty of refactoring our code. Although the code is presented in a single notebook, keep in mind that what I am essentially doing is modularizing. I won't claim that I am the most organized with my code, but I tried to keep it consistent with the best practices that I have observed others using with their open projects. Moving forward we will keep this structure becuase it is clearly superior to the organization that I had used before. </p>"},{"location":"blog/2016/07/18/a-tour-through-tensorflow-with-financial-data/#lstm-notebook","title":"LSTM (notebook)","text":"<p>My favorite neural network, and a true stepping stone into real deep learning is the long short-term memory network, or LSTM. Colah wrote an incredibly clear explanation of LSTM and there is really no substitute to reading his post. To describe the setup as briefly as possible, you input the data one timestep at a time to the LSTM cell. And each timestep the cell not only recieves the new input, but it recieves the last timestep's output and what is called the cell state, a vector that carries information about what happened in the past. Within the cell you have trained gates (basically small neural nets) that decide, based on the three inputs, what to forget from the past cell state, what to remember (or add) to the new state, and what to output this timestep. It is a very powerful tool and fascinating in how effective it is. </p> <p>I am now pretty far into this series and I have a pretty good idea of where it will go from here. These are the problems that I must tackle in no particular order: * new policies such as     + long/short equality amongs two symbols and more     + spread trading (if that is different from above)     + minimize correlation/ new risk meaesure that is appropriate for large number of symbols * migrating to AWS and using GPU computing power * ensebling large numbers of strategies that are generated with the same code     + policy grads find local maxima so no reason not to use that to my advantage * testing suite to be able to test if the strategies are viable objectively * convolution nets, especially among larger groups of symbols we can expect that some patterns are fractal * turning it into a more formal project or web app</p> <p>And of course we can start moving to other sources of data: * text * scraping * games</p> <p>Stay tuned for some articles that I will write about the algorithms used here and a discussion of the difficulties of using these techniques for algorithmic trading developement.  </p> <p>1: I hope you didnt sleep through Linear Algebra class! I owe all my LA skills to Comrade Otto Bretscher of Colby College whose class I did sleep through but whose text book is worth its weight in gold.</p> <p>Share on </p>"},{"location":"services/","title":"Consulting Services - Applied Machine Learning and AI","text":"<p>I help companies apply AI and ML technology to their most pressing business problems. With a decade of experience as a machine learning engineer, I bring deep expertise and a commitment to high standards that ensures project success.</p> <p>My experience at a successful SaaS startup and a top consulting firm has given me a unique blend of technical and business skills. I pride myself on my ability to engage deeply on both sides of the equation, and this focus has won me a proven track record of addressing complex challenges for clients.</p>"},{"location":"services/#offerings","title":"Offerings","text":""},{"location":"services/#workshops-and-strategy","title":"Workshops and Strategy","text":""},{"location":"services/#ai-ideation-and-feasibility-workshop","title":"AI Ideation and Feasibility Workshop","text":"<p>Explore potential use cases and application areas for AI in your business, in terms of value and level-of-effort. </p>"},{"location":"services/#ai-roadmap-planning","title":"AI Roadmap Planning","text":"<p>Define a roadmap based on your company's AI strategic vision and current capabilities.</p>"},{"location":"services/#technical-coaching","title":"Technical Coaching","text":""},{"location":"services/#knowledge-intensive-application-design","title":"Knowledge-Intensive Application Design","text":"<p>Make the right product and architecture decisions to build GenAI applications that are reliable and valuable.</p>"},{"location":"services/#genai-rag-performance-strategy","title":"GenAI RAG Performance Strategy","text":"<p>Define a technical roadmap for improving the performance of GenAI applications by adopting a data-driven approach evaluation and measurement.</p>"},{"location":"services/#implementation-and-development","title":"Implementation and Development","text":""},{"location":"services/#genai-application-development","title":"GenAI Application Development","text":"<p>Ground an LLM agent in real-world data using the retrieval-augmented generation (RAG) architecture pattern.</p>"},{"location":"services/#ml-model-development","title":"ML Model Development","text":"<p>Develop machine learning models for forecasting, classification or regression. </p>"},{"location":"services/#geospatial-data-science","title":"Geospatial Data Science","text":"<p>Leverage geospatial data to gain insights and make data-driven decisions.</p>"},{"location":"services/genai_app_dev/","title":"GenAI RAG Application Development","text":""},{"location":"services/genai_app_dev/#objective","title":"Objective","text":"<p>Ground an LLM agent in real-world data using the retrieval-augmented generation (RAG) architecture pattern. </p>"},{"location":"services/genai_app_dev/#description","title":"Description","text":"<p>RAG applications come in many forms, from citation-enhanced chatbots to complex research assistants. They can be productivity tools for knowledge workers, business intelligence tools for decision makers, or product offerings for customers. </p> <p>Building on a strong foundation of ML application development, we've mastered the art of applying new LLM technology to knowledge-intensive applications. We are among the few who have gone beyond proof of concept technical demos, and learned the hard-won lessons that come from deploying these systems at scale. As we've learned, GenAI products become truly valuable when they reach a threshold of reliability. Building a reliable application out of unreliable components is our specialty. </p>"},{"location":"services/genai_app_dev/#engagement-details","title":"Engagement Details","text":""},{"location":"services/genai_app_dev/#key-activities","title":"Key Activities","text":"<ul> <li>Discovery: We'll work with you to assess data sources, define the application requirements, and align on solution architecture and design.</li> <li>Implementation: We'll rapidly develop and deploy an application, and continue to iteratively improve based on performance and feedback.</li> <li>Enablement: With comprehensive documentation and operational guides, we'll hand over the system ownership to your team, enabling them to operate and extend it.</li> </ul>"},{"location":"services/genai_app_dev/#deliverables","title":"Deliverables","text":"<ul> <li>Full stack GenAI application and source code</li> <li>Performance evaluation system for confidently estimating the reliability of the system</li> <li>Operational documentation and handover</li> </ul>"},{"location":"services/genai_app_dev/#highlights","title":"Highlights","text":"<ul> <li>Cloud agnostic</li> <li>LLM provider agnostic</li> </ul>"},{"location":"services/genai_rag_performance_strat/","title":"GenAI knowledge base performance strategy","text":""},{"location":"services/genai_rag_performance_strat/#objective","title":"Objective","text":"<p>Define a technical roadmap for improving the performance of knowledge base GenAI applications by adopting a data-driven approach evaluation and measurement. </p>"},{"location":"services/genai_rag_performance_strat/#description","title":"Description","text":"<p>Reliability is key to the value proposition of RAG-based application products. However, these applications are often plagued in production by quality problems such as hallucinations and other sub-optimal responses. By adopting a data-driven approach to evaluation, observability and development priorities, technical teams can build reliable systems out of unreliable components. </p> <p>With a decade of experience developing classic ML applications, and significant hands-on experience with GenAI applications in the last two years, Liam is a valuable advisor in addressing this long tail of necessary improvements for your GenAI knowledge base system. He takes a practical approach, defining a custom roadmap for your development team that prioritizes quick wins while laying the foundation for sophisticated domain-specific solutions. </p> <p>The discussions will also cover the best practices and tools for knowledge system development that top teams have adopted to optimize development velocity and shorten iteration cycles. With code examples and detailed diagrams, I define a standard of excellence for this new type of system that results in a significant improvement in response quality that end users experience. </p>"},{"location":"services/genai_rag_performance_strat/#engagement-details","title":"Engagement Details","text":""},{"location":"services/genai_rag_performance_strat/#key-activities","title":"Key Activities","text":"<p>Workshops: Through a series of discovery workshops, We'll review your current application, RAG approaches, evaluation systems and development processes. We'll describe best practices for the development team to adopt. </p> <p>Design: Based on your input, we'll design additional system components, processes and logical application flows for improving your GenAI system performance. </p> <p>Roadmap: Following our design collaboration, we'll summarize the current state of your GenAI application, identify the top priority features and initiatives to implement, and craft an implementation plan.</p>"},{"location":"services/genai_rag_performance_strat/#deliverables","title":"Deliverables","text":"<ul> <li>GenAI knowledge base performance workshops</li> <li>Custom roadmap and implementation plan for improving your GenAI knowledge base performance</li> <li>Summary of best practices and tools applicable to your application All diagrams and documentation developed during the engagement including source code, scripts, templates, and other technical artifacts</li> </ul>"},{"location":"services/technical_expertise/","title":"Technical expertise","text":""},{"location":"services/technical_expertise/#technical-expertise","title":"Technical Expertise","text":""},{"location":"services/technical_expertise/#genai-application-development-ai-engineering","title":"GenAI Application Development (AI Engineering)","text":"<p>\u201cAI Engineering\u201d is a term being used for the new domain around applying LLMs (open source / from providers) to develop the new application types that the technology unlocks - knowledge systems, content generation, workflow automation, semantic parsing and querying, and others.</p> <p>BCG invested an enormous amount of capacity in 2023 in order to get its staff and leadership hands-on experience in this area. I had the opportunity to develop demos and lead the development of several applications:</p> <ul> <li>Consumer-facing knowledge assistant product at a global B2B information services provider</li> <li>GenAI-enabled network incident trend detection for a large telecommunications company</li> <li>Agricultural industry strategic knowledge assistant</li> <li>Natural language geospatial query generation and visualization application for agronomists</li> </ul>"},{"location":"services/technical_expertise/#machine-learning-application-development","title":"Machine Learning Application Development","text":"<p>Most of my career has been around the development of data-intensive applications that leverage Machine Learning. I have deep experience in ML modeling techniques as well as the practical details of ML pipelines and the deployment of these systems to cloud services for scalability and effective hosting.</p> <p>Some examples of projects in this area: * Application and computer vision pipeline for leading insurance company to automate estimations of coverage in force (i.e., potential liabilities) * Crop yield forecast for US corn and soy that beat the USDA in a historical backtest and live during the 2023 growing season * Designed and trained a Reinforcement Learning agent to optimize machine operations at a food processing plant, using model-based and model-free RNN-based architecture.</p>"},{"location":"services/technical_expertise/#geospatial-data-science","title":"Geospatial Data Science","text":"<p>As the founding tech lead of BCG\u2019s Center for Earth Intelligence, I had the opportunity to gain deep expertise in the field of geospatial data science. Using a scalable tech stack in python in order to achieve the highest modeling expressibility, we applied advanced ML and computer vision techniques to petabyte-scale data across continents. We took on use cases in insurance assessments, climate risk modeling, agronomy, B2B agriculture sales, and supply chain simulation.</p>"},{"location":"services/technical_expertise/#technical-strategy-for-ai-and-ml","title":"Technical Strategy for AI and ML","text":"<p>At BCG, I had several opportunities to leverage my deep practical technical expertise in an advisory role. My communication and critical thinking skills gave me a natural aptitude for this type of work and I found it highly stimulating. Some examples: * Facilitated tech workshops with C-suite technologists across industries to define 2-year GenAI roadmaps. * Conducted technical audits and strategic reviews of tech startups. * Defined operating model for team of 8 experts to deliver value in new domain (geospatial DS). * Enabled the development of a cross-functional data science team at a consumer goods company by defining roles and guiding re-skilling efforts. * Weighed in on the feasibility of unproven use cases in GenAI and ML.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2016/","title":"2016","text":""}]}